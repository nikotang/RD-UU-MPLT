{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmt46QqZ8QDT2yDOg0OCWD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikotang/RD-UU-MPLT/blob/main/RD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R&D: Additional Paraphrase Training Drives Language Models Closer to Human Behaviour on Natural Language Inference"
      ],
      "metadata": {
        "id": "8CBL5bZ-Nmjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repo: https://github.com/nikotang/RD-UU-MPLT/\n",
        "\n",
        "TLDR: Language models predict 'very well' on NLI hypotheses with randomised word order, which does and doesn't make sense. I think it shouldn't be so confident after seeing a paraphrase dataset (PAWS) and an anaphora resolution dataset (Winogrande).\n",
        "\n",
        "Winogrande didn't work out eventually. PAWS did.\n",
        "\n",
        "Notes:\n",
        "\n",
        "RoBERTa large is only a bit better than base on MNLI. RoBERTa large finetuned on NLI did no better than just large base: (https://github.com/facebookresearch/fairseq/tree/main/examples/roberta)\n",
        "\n",
        "Finetuning roberta on winogrande: (https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/wsc/README.md)\n"
      ],
      "metadata": {
        "id": "WYwAUdEyNmoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0: Installations and imports"
      ],
      "metadata": {
        "id": "8CH_00dTpeh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq accelerate datasets evaluate nltk transformers"
      ],
      "metadata": {
        "id": "SAlrkKdl7VFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZJX-ipONlvg"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict, Value, load_dataset, concatenate_datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification, EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: Preprocessing data\n",
        "\n",
        "If this is run before or if the processed data from the Github repo is downloaded, go to [section 1.5](#section-1.5).\n",
        "\n",
        "HANS isn't used eventually."
      ],
      "metadata": {
        "id": "XDoyHT0Wpnxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-NLI"
      ],
      "metadata": {
        "id": "Nnm0KKvpqEVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load from Huggingface dataset hub\n",
        "mnli_dataset = load_dataset('multi_nli')\n",
        "\n",
        "# remove irrelevant columns\n",
        "mnli_dataset = mnli_dataset.remove_columns(['promptID', 'pairID', 'premise_binary_parse', 'premise_parse', \\\n",
        "                                            'hypothesis_binary_parse', 'hypothesis_parse', 'genre'])\n",
        "# rename some columns\n",
        "mnli_dataset = mnli_dataset.rename_column('premise', 'sentence1')\n",
        "mnli_dataset = mnli_dataset.rename_column('hypothesis', 'sentence2')"
      ],
      "metadata": {
        "id": "3vGIs8VTOtLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a test set\n",
        "matched = mnli_dataset['validation_matched'].train_test_split(test_size=0.5)\n",
        "mismatched = mnli_dataset['validation_mismatched'].train_test_split(test_size=0.5)\n",
        "\n",
        "del mnli_dataset['validation_matched']\n",
        "del mnli_dataset['validation_mismatched']\n",
        "\n",
        "mnli_dataset['validation'] = concatenate_datasets([matched['train'], mismatched['train']])\n",
        "mnli_dataset['test'] = concatenate_datasets([matched['test'], mismatched['test']])"
      ],
      "metadata": {
        "id": "q8544YhX719q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# relabel the dataset: merge contradiction and neutral\n",
        "label2id = {'contradiction': 0, 'neutral': 0, 'entailment': 1}\n",
        "mnli_dataset = mnli_dataset.align_labels_with_mapping(label2id, 'label')"
      ],
      "metadata": {
        "id": "Yj5Bj8xL2LuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trim training set, 400,000 is too much\n",
        "# sometimes doens't work without reimporting\n",
        "from datasets import Dataset\n",
        "mnli_dataset['train'] = Dataset.from_dict(mnli_dataset['train'][:100000])"
      ],
      "metadata": {
        "id": "TaN0b4UuOc6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PAWS"
      ],
      "metadata": {
        "id": "_4h6f8i6qOj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paws_dataset = load_dataset('paws', 'labeled_final')\n",
        "paws2 = load_dataset('paws', 'labeled_swap')\n",
        "\n",
        "paws_dataset['train'] = concatenate_datasets([paws_dataset['train'], paws2['train']])\n",
        "\n",
        "paws_dataset = paws_dataset.remove_columns('id')"
      ],
      "metadata": {
        "id": "h3STH2tr97wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Winogrande\n",
        "\n",
        "The dataset uses a '_' for the place where the anaphora should be. To create premise-hypothesis pairs, a pronoun is predicted (here with the Huggingface fill-mask pipeline, DistilRoBERTa-base by default) and put in place. An entailed hypothesis will be putting the correct noun in place, and the other noun contender put in place makes the non-entailment hypothesis.\n",
        "\n",
        "Or skip to the next following subsubsection to download the filled dataset directly."
      ],
      "metadata": {
        "id": "eHk8NqYKqXHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -Nq https://storage.googleapis.com/ai2-mosaic/public/winogrande/winogrande_1.1.zip\n",
        "!unzip winogrande_1.1.zip; rm -rf __MACOSX\n",
        "!pip install nvidia-ml-py3 tqdm\n",
        "\n",
        "import csv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "49c1wIlNV1jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_premises = []\n",
        "with open('winogrande_1.1/train_xl.jsonl') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        train_premises.append(data[\"sentence\"])\n",
        "\n",
        "dev_premises = []\n",
        "with open('winogrande_1.1/dev.jsonl') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        dev_premises.append(data[\"sentence\"])"
      ],
      "metadata": {
        "id": "vkuRWdY5eUq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = pipeline(\"fill-mask\", device=0)\n",
        "targets=['Ġhe', 'Ġhim', 'Ġhis',\n",
        "            'Ġshe', 'Ġher', 'Ġhers',\n",
        "            'Ġit', 'Ġits', 'Ġthey', 'Ġtheir', 'Ġthem']\n",
        "\n",
        "def add_mask(premises):\n",
        "    # replace _ with mask token\n",
        "    for i, sent in enumerate(premises):\n",
        "        if 'the _' in sent:\n",
        "            premises[i] = sent.split('the _')\n",
        "        elif '_' in sent:\n",
        "            premises[i] = sent.split('_')\n",
        "        else:\n",
        "            print(i)\n",
        "        premises[i] = '<mask>'.join(sent) # mask token for roberta\n",
        "    return premises\n",
        "\n",
        "def predict_pronouns(masked_premises)\n",
        "    filled_premises = []\n",
        "    for line in tqdm(masked_premises):\n",
        "        guess = predictor(line, targets=targets, top_k=1)\n",
        "        filled_premises.append(guess[0]['sequence'])\n",
        "    return filled_premises\n",
        "\n",
        "def write_pairs_to_file(filled_premises, file_input, file_output):\n",
        "    with open('dev_transformed.tsv', 'w') as trans_f:\n",
        "        with open('dev.jsonl', 'r') as f:\n",
        "            tsv_writer = csv.writer(trans_f, delimiter='\\t')\n",
        "            tsv_writer.writerow(['id', 'sentence1', 'sentence2', 'label'])\n",
        "            id = 1\n",
        "            for i, line in enumerate(f):\n",
        "                data = json.loads(line)\n",
        "                s_segments = data[\"sentence\"].split('_')\n",
        "                correct = data[\"answer\"]\n",
        "                wrong = ['1', '2']\n",
        "                wrong.remove(correct)\n",
        "                wrong = wrong[0]\n",
        "\n",
        "                correct_s = data[f\"option{correct}\"].join(s_segments)\n",
        "                tsv_writer.writerow([id, filled_premises[i], correct_s, '1'])\n",
        "                id += 1\n",
        "                wrong_s = data[f\"option{wrong}\"].join(s_segments)\n",
        "                tsv_writer.writerow([id, filled_premises[i], wrong_s, '0'])\n",
        "                id += 1"
      ],
      "metadata": {
        "id": "bUqqseRna9HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_masked = add_mask(train_premises)\n",
        "train_filled = predict_pronouns(train_masked)\n",
        "write_pairs_to_file(train_filled, 'winogrande_1.1/train_xl.jsonl', 'train_xl_transformed.tsv')\n",
        "\n",
        "dev_masked = add_mask(dev_premises)\n",
        "dev_filled = predict_pronouns(dev_masked)\n",
        "write_pairs_to_file(dev_filled, 'winogrande_1.1/dev.jsonl', 'dev_transformed.tsv')"
      ],
      "metadata": {
        "id": "udZOcQzigYLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After filling in predicted pronouns"
      ],
      "metadata": {
        "id": "7nBwYYbdYuAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download Winogrande with pronouns filled in\n",
        "!wget -Nq https://raw.githubusercontent.com/nikotang/RD-UU-MPLT/main/winogrande_1.1/train_xl_transformed.tsv\n",
        "!wget -Nq https://raw.githubusercontent.com/nikotang/RD-UU-MPLT/main/winogrande_1.1/dev_transformed.tsv"
      ],
      "metadata": {
        "id": "kusdC8NihMLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('train_xl_transformed.tsv', sep='\\t')\n",
        "winogrande_train = Dataset.from_pandas(df_train)\n",
        "winogrande_dataset = winogrande_train.train_test_split(test_size=0.1)\n",
        "\n",
        "df_dev = pd.read_csv('dev_transformed.tsv', sep='\\t')\n",
        "winogrande_dataset['validation'] = Dataset.from_pandas(df_dev)\n",
        "winogrande_dataset = winogrande_dataset.remove_columns('id')"
      ],
      "metadata": {
        "id": "NavGrimd-PsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HANS\n",
        "\n",
        "This set isn't used."
      ],
      "metadata": {
        "id": "Bh1WFpafTUHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hans_dataset = load_dataset('hans')\n",
        "\n",
        "# hans_dataset = hans_dataset.remove_columns(['parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'subcase', 'template'])\n",
        "\n",
        "# # rename some columns\n",
        "# hans_dataset = hans_dataset.rename_column('premise', 'sentence1')\n",
        "# hans_dataset = hans_dataset.rename_column('hypothesis', 'sentence2')"
      ],
      "metadata": {
        "id": "ZyWUqz08TXBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # make test set\n",
        "# val_test = hans_dataset['validation'].train_test_split(test_size=0.5) # shuffle default=True\n",
        "\n",
        "# hans_dataset['validation'] = val_test['train']\n",
        "# hans_dataset['test'] = val_test['test']"
      ],
      "metadata": {
        "id": "sNdYU_62UUgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# relabel the dataset\n",
        "# label2id = {'non-entailment': 0, 'entailment': 1}\n",
        "# hans_dataset = hans_dataset.align_labels_with_mapping(label2id, 'label')"
      ],
      "metadata": {
        "id": "CwiAIj7UUyGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for h in ('lexical_overlap', 'subsequence', 'constituent'):\n",
        "#   print(f'{h}: {hans_dataset['validation']['heuristic'].count(h)}')"
      ],
      "metadata": {
        "id": "YsM1foClV9aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "bWOeq8x_5Am5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "9YWOfiRKGinq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(ds):\n",
        "  return tokenizer(ds['sentence1'], ds['sentence2'], padding=True, return_tensors='pt')\n",
        "\n",
        "def process(dataset_d):\n",
        "  return dataset_d.map(tokenize, batched=True, batch_size=64, num_proc=4)\n",
        "\n",
        "# tokenize datasets\n",
        "mnli_dataset_tok = process(mnli_dataset)\n",
        "paws_dataset_tok = process(paws_dataset)\n",
        "winogrande_dataset_tok = process(winogrande_dataset)\n",
        "# hans_dataset_tok = process(hans_dataset)\n",
        "\n",
        "# rename 'label' to 'labels' for Trainer (alternatively specify 'label' in Trainer params)\n",
        "mnli_dataset_tok = mnli_dataset_tok.rename_column('label', 'labels')\n",
        "paws_dataset_tok = paws_dataset_tok.rename_column('label', 'labels')\n",
        "winogrande_dataset_tok = winogrande_dataset_tok.rename_column('label', 'labels')\n",
        "# hans_dataset_tok = hans_dataset_tok.rename_column('label', 'labels')\n",
        "\n",
        "for dataset_d in [mnli_dataset_tok, paws_dataset_tok, winogrande_dataset_tok]: # hans_dataset_tok\n",
        "  dataset_d.set_format(type='torch', columns=['labels'], output_all_columns=True)"
      ],
      "metadata": {
        "id": "ogMRgfMlkSbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnli_dataset_tok.save_to_disk('./mnli_datasets')\n",
        "paws_dataset_tok.save_to_disk('./paws_datasets')\n",
        "winogrande_dataset_tok.save_to_disk('./winogrande_datasets')\n",
        "# hans_dataset_tok.save_to_disk('./hans_datasets')"
      ],
      "metadata": {
        "id": "MDsV1sQZ5jaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r mnli.zip mnli_datasets\n",
        "!zip -r paws.zip paws_datasets\n",
        "!zip -r winogrande.zip winogrande_datasets\n",
        "# !zip -r hans.zip hans_datasets"
      ],
      "metadata": {
        "id": "cddsXWUw7N09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create randomised data\n",
        "\n",
        "We want to have test sets that have random word order to test changes in confidence after finetuning the language model. A sorted set and a shuffled set are created for each test set."
      ],
      "metadata": {
        "id": "XqAo_fp9XLWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def randomise(ds):\n",
        "  ts = ds.remove_columns(['input_ids', 'attention_mask'])\n",
        "  ts_sort = ts.map(lambda x: {'sentence2': ' '.join(sorted(word_tokenize(x['sentence2'])))})\n",
        "  ts_shuff = ts.map(lambda x: {'sentence2': ' '.join(sample(word_tokenize(x['sentence2']), \\\n",
        "                                                            len(word_tokenize(x['sentence2']))))}) # random.shuffle is in-place\n",
        "  return ts_sort, ts_shuff"
      ],
      "metadata": {
        "id": "jP0TCNuLYUaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnli_sort, mnli_shuff = randomise(mnli_dataset_tok['test'])\n",
        "paws_sort, paws_shuff = randomise(paws_dataset_tok['test'])\n",
        "winogrande_sort, winogrande_shuff = randomise(winogrande_dataset_tok['test'])\n",
        "# hans_sort, hans_shuff = randomise(hans_dataset_tok['test'])"
      ],
      "metadata": {
        "id": "O57mZItIYYe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize datasets\n",
        "mnli_sort = process(mnli_sort)\n",
        "paws_sort = process(paws_sort)\n",
        "winogrande_sort = process(winogrande_sort)\n",
        "mnli_shuff = process(mnli_shuff)\n",
        "paws_shuff = process(paws_shuff)\n",
        "winogrande_shuff = process(winogrande_shuff)"
      ],
      "metadata": {
        "id": "fzJbUKfVYbk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save them in one directory\n",
        "!mkdir randoms\n",
        "\n",
        "mnli_sort.save_to_disk('./randoms/mnli_sort')\n",
        "paws_sort.save_to_disk('./randoms/paws_sort')\n",
        "winogrande_sort.save_to_disk('./randoms/winogrande_sort')\n",
        "mnli_shuff.save_to_disk('./randoms/mnli_shuff')\n",
        "paws_shuff.save_to_disk('./randoms/paws_shuff')\n",
        "winogrande_shuff.save_to_disk('./randoms/winogrande_shuff')"
      ],
      "metadata": {
        "id": "zLPXI2zMYe51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r randoms.zip randoms"
      ],
      "metadata": {
        "id": "nZ1gtwKsYhaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-1.5\"></a>\n",
        "# OR 1.5 load already preprocessed data from here"
      ],
      "metadata": {
        "id": "pSndSbMl61-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -Nq https://github.com/nikotang/RD-UU-MPLT/raw/main/mnli.zip\n",
        "!wget -Nq https://github.com/nikotang/RD-UU-MPLT/raw/main/paws.zip\n",
        "!wget -Nq https://github.com/nikotang/RD-UU-MPLT/raw/main/winogrande.zip\n",
        "!wget -Nq https://github.com/nikotang/RD-UU-MPLT/raw/main/randoms.zip\n",
        "\n",
        "!unzip mnli.zip\n",
        "!unzip paws.zip\n",
        "!unzip winogrande.zip\n",
        "!unzip randoms.zip"
      ],
      "metadata": {
        "id": "jmT4XFW53D1k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "mnli_dataset_tok = load_from_disk('./mnli_datasets')\n",
        "paws_dataset_tok = load_from_disk('./paws_datasets')\n",
        "winogrande_dataset_tok = load_from_disk('./winogrande_datasets')\n",
        "\n",
        "mnli_sorted = load_from_disk('./randoms/mnli_sort')\n",
        "mnli_shuffled = load_from_disk('./randoms/mnli_shuff')\n",
        "paws_sorted = load_from_disk('./randoms/paws_sort')\n",
        "paws_shuffled = load_from_disk('./randoms/paws_shuff')"
      ],
      "metadata": {
        "id": "nXBqyuaeXNAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change data type for the PAWS set to match MNLI\n",
        "# Somehow the change isn't preserved if done before loading to and from disk\n",
        "paws_dataset_tok = paws_dataset_tok.cast_column('labels', Value(dtype='int64'))"
      ],
      "metadata": {
        "id": "bJrFa0YvIGNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnli_paws_dataset_tok = concatenate_datasets([mnli_dataset_tok['train'], paws_dataset_tok['train']])"
      ],
      "metadata": {
        "id": "5ET3rDkYHYws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make mini sets for testing\n",
        "\n",
        "mnli_mini = Dataset.from_dict(mnli_dataset_tok['train'][:10000])\n",
        "paws_mini = Dataset.from_dict(paws_dataset_tok['train'][:10000])\n",
        "winogrande_mini = Dataset.from_dict(winogrande_dataset_tok['train'][:10000])\n",
        "mnli_paws_mini = Dataset.from_dict(mnli_paws_dataset_tok.shuffle()[:10000]) # contains only a training set, not a DatasetDict\n",
        "\n",
        "mnli_val_mini = Dataset.from_dict(mnli_dataset_tok['validation'][:2000])"
      ],
      "metadata": {
        "id": "X0SYzAOM3vrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Training"
      ],
      "metadata": {
        "id": "akSh-SPSqA3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "ybQrBG1Vm6y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick the set to train on, and file name to save\n",
        "\n",
        "train_dataset = mnli_paws_dataset_tok\n",
        "eval_dataset = mnli_dataset_tok['validation']\n",
        "filename = 'mnli_paws'"
      ],
      "metadata": {
        "id": "QiIElcDblsR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base').to('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'./{filename}_model',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=1000,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=5e-4,\n",
        "    logging_dir=f'./{filename}_logs',\n",
        "    logging_steps=1000,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy'      # determine 'best' according to val acc\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]      # checks 3 more steps before early stopping\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "rBKZD6h1lLuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Evaluate"
      ],
      "metadata": {
        "id": "fTySlbZ-dLso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'mnli_paws'"
      ],
      "metadata": {
        "id": "Dx_CECyujTJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = RobertaForSequenceClassification.from_pretrained(f'./{filename}_model').to('cuda')\n",
        "\n",
        "test_args = TrainingArguments(\n",
        "    output_dir = f'./{filename}_results',\n",
        "    do_train = False,\n",
        "    do_predict = True,\n",
        "    per_device_eval_batch_size = 64\n",
        ")\n",
        "\n",
        "tester = Trainer(\n",
        "              model = test_model,\n",
        "              args = test_args,\n",
        "              compute_metrics = compute_metrics)\n",
        "\n",
        "tester.evaluate(eval_dataset=mnli_dataset_tok['test'])"
      ],
      "metadata": {
        "id": "rGBCSmGyOsv6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get logits to calculate confidence\n",
        "logits, references, _ = trainer.predict(mnli_dataset_tok['test'])"
      ],
      "metadata": {
        "id": "daa7G8Pe8BTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "from collections import Counter\n",
        "\n",
        "softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "soft_logits = [softmax(torch.tensor(logit)) for logit in logits]\n",
        "soft_logits = np.stack(np.array(soft_logits, dtype=object))\n",
        "\n",
        "preds = np.argmax(soft_logits, axis=1)\n",
        "pred_probs = np.amax(soft_logits, axis=1)"
      ],
      "metadata": {
        "id": "Lsuqqspirb8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the prediction results and probabilities\n",
        "\n",
        "# results = np.concatenate((pred_probs, preds, references), axis=1)\n",
        "# with open('results.npy', 'wb') as f:\n",
        "#   np.save(f, results)\n",
        "\n",
        "# to load the file:\n",
        "# with open('results.npy', 'rb') as f:\n",
        "#   results = np.load(f)"
      ],
      "metadata": {
        "id": "ica_7wy7L8p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_confidence = np.mean(pred_probs)\n",
        "print(f'Average confidence: {avg_confidence}')\n",
        "\n",
        "confidence_1 = []\n",
        "confidence_0 = []\n",
        "for i, prob in enumerate(pred_probs):\n",
        "  if preds[i] == 1:\n",
        "    confidence_1.append(prob)\n",
        "  else:\n",
        "    confidence_0.append(prob)\n",
        "\n",
        "print(f'Label count: {Counter(preds)}')\n",
        "print(f'Average confidence in Entailment predictions: {mean(confidence_1)}')\n",
        "print(f'Average confidence in Non-Entailment predictions: {mean(confidence_0)}')"
      ],
      "metadata": {
        "id": "qtJiGV5Tpg0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}